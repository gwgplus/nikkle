# yolo_ocr.py
# -*- coding: utf-8 -*-

import os
import cv2
import torch
import time
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
from PIL import Image, ImageOps

from ultralytics import YOLO
from transformers import TrOCRProcessor, VisionEncoderDecoderModel


class YOLOOCR:
    """YOLO + TrOCR OCR è™•ç†é¡åˆ¥"""
    
    def __init__(self, 
                 yolo_weights: str = "./best.pt",
                 model_dir: str = "./trocr-384x384-finetuned",
                 processor_dir: str = "./trocr-384x384-processor",
                 stage1_w: int = 360,
                 stage1_h: int = 128,
                 stage1_fill: Tuple[int, int, int] = (0, 0, 0),
                 stage2_size: int = 384,
                 stage2_fill: Tuple[int, int, int] = (255, 255, 255),
                 gen_max_len: int = 32,
                 gen_beams: int = 1,
                 use_fp16: bool = True,
                 optimize_memory: bool = True):
        """
        åˆå§‹åŒ– YOLO OCR è™•ç†å™¨
        
        Args:
            yolo_weights: YOLO æ¨¡å‹æ¬Šé‡æª”æ¡ˆè·¯å¾‘
            model_dir: TrOCR æ¨¡å‹ç›®éŒ„
            processor_dir: TrOCR è™•ç†å™¨ç›®éŒ„
            stage1_w: ç¬¬ä¸€éšæ®µå¯¬åº¦
            stage1_h: ç¬¬ä¸€éšæ®µé«˜åº¦
            stage1_fill: ç¬¬ä¸€éšæ®µå¡«å……é¡è‰²
            stage2_size: ç¬¬äºŒéšæ®µå°ºå¯¸
            stage2_fill: ç¬¬äºŒéšæ®µå¡«å……é¡è‰²
            gen_max_len: ç”Ÿæˆæœ€å¤§é•·åº¦
            gen_beams: ç”ŸæˆæŸæœç´¢æ•¸é‡ (1=greedy, æ›´å¿«)
            use_fp16: æ˜¯å¦ä½¿ç”¨ FP16 ç²¾åº¦ (GPU å„ªåŒ–)
            optimize_memory: æ˜¯å¦å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
        """
        self.yolo_weights = yolo_weights
        self.model_dir = model_dir
        self.processor_dir = processor_dir
        self.stage1_w = stage1_w
        self.stage1_h = stage1_h
        self.stage1_fill = stage1_fill
        self.stage2_size = stage2_size
        self.stage2_fill = stage2_fill
        self.gen_max_len = gen_max_len
        self.gen_beams = gen_beams
        self.use_fp16 = use_fp16
        self.optimize_memory = optimize_memory
        
        # æ¨¡å‹å’Œè¨­å‚™
        self.det_model = None
        self.processor = None
        self.trocr_model = None
        self.device = None
        self._initialized = False
        
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå»¶é²è¼‰å…¥ï¼‰"""
        if self._initialized:
            return
            
        try:
            print(">> Loading YOLO...")
            self.det_model = YOLO(self.yolo_weights)
            
            print(">> Loading TrOCR...")
            self.processor = TrOCRProcessor.from_pretrained(self.processor_dir)
            
            # âœ… ä¿®æ­£ï¼šè¨­å®šåˆæ³• sizeï¼Œé¿å… ValueError
            if hasattr(self.processor, "image_processor"):
                ip = self.processor.image_processor
                ip.do_center_crop = False
                ip.do_resize = False
                ip.size = {"height": self.stage2_size, "width": self.stage2_size}
            
            # âœ… ä½¿ç”¨ FP16 åŠ é€Ÿæ¨è«–
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.trocr_model = VisionEncoderDecoderModel.from_pretrained(self.model_dir)
            
            if self.use_fp16 and self.device.type == "cuda":
                self.trocr_model = self.trocr_model.half()
                print(">> Using FP16 precision for GPU acceleration")
            
            self.trocr_model.to(self.device).eval()
            
            # è¨˜æ†¶é«”å„ªåŒ–
            if self.optimize_memory and self.device.type == "cuda":
                torch.cuda.empty_cache()
                print(">> GPU memory optimized")
            
            self._initialized = True
            print(">> Models loaded successfully")
            
        except Exception as e:
            raise RuntimeError(f"Failed to initialize models: {str(e)}")
    
    def _letterbox_36128(self, img: Image.Image) -> Image.Image:
        """Stage1ï¼šç­‰æ¯”ç¸®æ”¾ + è£œé‚Šåˆ°æŒ‡å®šå°ºå¯¸"""
        img = img.convert("RGB")
        r = min(self.stage1_w / img.width, self.stage1_h / img.height)
        nw, nh = max(1, int(round(img.width * r))), max(1, int(round(img.height * r)))
        resized = img.resize((nw, nh), Image.LANCZOS)
        dw, dh = self.stage1_w - nw, self.stage1_h - nh
        padding = (dw // 2, dh // 2, dw - dw // 2, dh - dh // 2)
        return ImageOps.expand(resized, padding, fill=self.stage1_fill)
    
    def _to_384_square(self, img_36128: Image.Image) -> Image.Image:
        """Stage2ï¼šæŠŠç¬¬ä¸€éšæ®µçµæœç½®ä¸­è²¼åˆ°æ­£æ–¹å½¢"""
        w, h = img_36128.size
        if w > self.stage2_size or h > self.stage2_size:
            s = min(self.stage2_size / w, self.stage2_size / h)
            img_36128 = img_36128.resize(
                (max(1, int(round(w * s))), max(1, int(round(h * s)))),
                Image.BICUBIC
            )
            w, h = img_36128.size
        canvas = Image.new("RGB", (self.stage2_size, self.stage2_size), self.stage2_fill)
        canvas.paste(img_36128, ((self.stage2_size - w)//2, (self.stage2_size - h)//2))
        return canvas
    
    def _clip_box(self, x1: float, y1: float, x2: float, y2: float, w: int, h: int) -> Tuple[int, int, int, int]:
        """è£å‰ªé‚Šç•Œæ¡†åˆ°åœ–ç‰‡ç¯„åœå…§"""
        x1 = max(0, min(int(x1), w - 1))
        y1 = max(0, min(int(y1), h - 1))
        x2 = max(0, min(int(x2), w - 1))
        y2 = max(0, min(int(y2), h - 1))
        return x1, y1, x2, y2
    
    def access_ocr(self, image_path: str) -> Dict[str, Any]:
        """
        å°å–®å¼µåœ–ç‰‡é€²è¡Œ OCR è™•ç†
        
        Args:
            image_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
            
        Returns:
            Dict åŒ…å«:
                - success: bool, æ˜¯å¦æˆåŠŸ
                - results: list, OCR çµæœåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«:
                    - bbox: [x1, y1, x2, y2], é‚Šç•Œæ¡†åº§æ¨™
                    - text: str, è­˜åˆ¥çš„æ–‡å­—
                    - confidence: float, ç½®ä¿¡åº¦
                - timing: dict, æ™‚é–“çµ±è¨ˆ:
                    - total_ms: float, ç¸½è™•ç†æ™‚é–“ (æ¯«ç§’)
                    - yolo_ms: float, YOLO æª¢æ¸¬æ™‚é–“ (æ¯«ç§’)
                    - trocr_ms: float, TrOCR è­˜åˆ¥æ™‚é–“ (æ¯«ç§’)
                    - text_count: int, è­˜åˆ¥çš„æ–‡å­—æ•¸é‡
                - error: str, éŒ¯èª¤è¨Šæ¯ï¼ˆå¦‚æœå¤±æ•—ï¼‰
        """
        try:
            # é–‹å§‹è¨ˆæ™‚
            start_time = time.perf_counter()
            
            # å»¶é²åˆå§‹åŒ–æ¨¡å‹
            self._initialize_models()
            
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                return {
                    "success": False,
                    "results": [],
                    "timing": {"total_ms": 0, "yolo_ms": 0, "trocr_ms": 0, "text_count": 0},
                    "error": f"Image file not found: {image_path}"
                }
            
            # è®€å–åœ–ç‰‡
            bgr = cv2.imread(image_path)
            if bgr is None:
                return {
                    "success": False,
                    "results": [],
                    "timing": {"total_ms": 0, "yolo_ms": 0, "trocr_ms": 0, "text_count": 0},
                    "error": f"Cannot read image: {image_path}"
                }
            
            # YOLO æª¢æ¸¬è¨ˆæ™‚
            yolo_start = time.perf_counter()
            results = self.det_model(image_path)[0]
            yolo_time = (time.perf_counter() - yolo_start) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
            
            # è½‰æ›ç‚º PIL åœ–ç‰‡
            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(rgb)
            
            ocr_results = []
            trocr_start = time.perf_counter()
            
            with torch.no_grad():
                for box in results.boxes:
                    cls_id = int(box.cls[0])
                    class_name = self.det_model.names.get(cls_id, str(cls_id))
                    
                    # åªè™•ç† 'text' é¡åˆ¥
                    if class_name != "text":
                        continue
                    
                    # å–å¾—é‚Šç•Œæ¡†åº§æ¨™
                    x1, y1, x2, y2 = box.xyxy[0].tolist()
                    x1, y1, x2, y2 = self._clip_box(x1, y1, x2, y2, pil_img.width, pil_img.height)
                    
                    # æª¢æŸ¥é‚Šç•Œæ¡†æœ‰æ•ˆæ€§
                    if x2 <= x1 or y2 <= y1:
                        continue
                    
                    # è£å‰ªæ–‡å­—å€åŸŸ
                    crop = pil_img.crop((x1, y1, x2, y2))
                    
                    # å…©æ®µå¼å¹¾ä½•è™•ç†
                    img_36128 = self._letterbox_36128(crop)
                    img_384 = self._to_384_square(img_36128)
                    
                    # TrOCR æ¨è«–ï¼ˆGPU å„ªåŒ–ï¼‰
                    inputs = self.processor(images=img_384, return_tensors="pt").to(self.device)
                    
                    # ğŸ§  å¼·åˆ¶è½‰ç‚º FP16ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                    if self.use_fp16 and self.device.type == "cuda":
                        inputs = {k: v.half() for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        pred_ids = self.trocr_model.generate(
                            **inputs,
                            max_length=self.gen_max_len,
                            num_beams=self.gen_beams,  # âš¡ greedy è§£ç¢¼ï¼Œæ›´å¿«
                            early_stopping=True
                        )
                    pred_text = self.processor.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()
                    
                    # å–å¾—ç½®ä¿¡åº¦
                    confidence = float(box.conf[0]) if hasattr(box, 'conf') else 0.0
                    
                    # æ·»åŠ åˆ°çµæœ
                    ocr_results.append({
                        "bbox": [x1, y1, x2, y2],
                        "text": pred_text,
                        "confidence": confidence
                    })
            
            # è¨ˆç®— TrOCR æ™‚é–“
            trocr_time = (time.perf_counter() - trocr_start) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
            total_time = (time.perf_counter() - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
            
            return {
                "success": True,
                "results": ocr_results,
                "timing": {
                    "total_ms": round(total_time, 2),
                    "yolo_ms": round(yolo_time, 2),
                    "trocr_ms": round(trocr_time, 2),
                    "text_count": len(ocr_results)
                },
                "error": None
            }
            
        except Exception as e:
            total_time = (time.perf_counter() - start_time) * 1000 if 'start_time' in locals() else 0
            return {
                "success": False,
                "results": [],
                "timing": {"total_ms": round(total_time, 2), "yolo_ms": 0, "trocr_ms": 0, "text_count": 0},
                "error": f"OCR processing failed: {str(e)}"
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """å–å¾—æ¨¡å‹è³‡è¨Š"""
        return {
            "yolo_weights": self.yolo_weights,
            "model_dir": self.model_dir,
            "processor_dir": self.processor_dir,
            "device": str(self.device) if self.device else "Not initialized",
            "initialized": self._initialized,
            "stage1_size": (self.stage1_w, self.stage1_h),
            "stage2_size": self.stage2_size,
            "use_fp16": self.use_fp16,
            "optimize_memory": self.optimize_memory,
            "gen_beams": self.gen_beams
        }


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # å‰µå»º OCR è™•ç†å™¨
    ocr = YOLOOCR()
    
    # è™•ç†å–®å¼µåœ–ç‰‡
    result = ocr.access_ocr("./test_image.jpg")
    
    if result["success"]:
        print("OCR è™•ç†æˆåŠŸ!")
        
        # é¡¯ç¤ºæ™‚é–“çµ±è¨ˆ
        timing = result["timing"]
        print(f"\nâ±ï¸  æ™‚é–“çµ±è¨ˆ:")
        print(f"  ç¸½è™•ç†æ™‚é–“: {timing['total_ms']} ms")
        print(f"  YOLO æª¢æ¸¬: {timing['yolo_ms']} ms")
        print(f"  TrOCR è­˜åˆ¥: {timing['trocr_ms']} ms")
        print(f"  è­˜åˆ¥æ–‡å­—æ•¸é‡: {timing['text_count']}")
        
        if timing['text_count'] > 0:
            avg_per_text = timing['trocr_ms'] / timing['text_count']
            print(f"  å¹³å‡æ¯æ–‡å­—: {avg_per_text:.2f} ms")
        
        # é¡¯ç¤ºè­˜åˆ¥çµæœ
        print(f"\nğŸ“ è­˜åˆ¥çµæœ:")
        for i, res in enumerate(result["results"]):
            print(f"  çµæœ {i+1}:")
            print(f"    æ–‡å­—: {res['text']}")
            print(f"    é‚Šç•Œæ¡†: {res['bbox']}")
            print(f"    ç½®ä¿¡åº¦: {res['confidence']:.3f}")
    else:
        print(f"OCR è™•ç†å¤±æ•—: {result['error']}")
        if "timing" in result:
            timing = result["timing"]
            print(f"è™•ç†æ™‚é–“: {timing['total_ms']} ms")
    
    # é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
    print("\næ¨¡å‹è³‡è¨Š:")
    info = ocr.get_model_info()
    for key, value in info.items():
        print(f"  {key}: {value}")
